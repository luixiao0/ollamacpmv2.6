name: Build llama.cpp (MiniCPM-V2.5)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  get_short_commit_hash:
    runs-on: ubuntu-latest
    outputs:
      hash: ${{ steps.short_commit_hash.outputs.short }}
    steps:
      - name: Prepare commit hash
        id: commit_hash
        run: |
          git clone https://github.com/OpenBMB/llama.cpp cache -b prepare-PR-of-minicpm-v2.5 --depth 1
          cd cache
          echo "hash=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT
          cd ..
          rm -rf cache
      - name: Get short commit hash
        id: short_commit_hash
        uses: prompt/actions-commit-hash@v3
        with:
          commit: ${{ steps.commit_hash.outputs.hash }}
  # build_intel_mkl_avx2_linux:
  #   needs: get_short_commit_hash
  #   runs-on: ubuntu-20.04
  #   steps:
  #     - name: Install Deps
  #       run: |
  #         wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
  #         echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
  #         sudo apt-get update
  #         sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
  #     - name: Checkout
  #       uses: actions/checkout@v4
  #       with:
  #         repository: OpenBMB/llama.cpp
  #         submodules: recursive
  #         ref: prepare-PR-of-minicpm-v2.5
  #     - name: Build
  #       run: |
  #         mkdir build
  #         cd build
  #         source /opt/intel/oneapi/setvars.sh
  #         cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_NATIVE=ON -DLLAMA_AVX2=ON
  #         cmake --build . --config Release -j $(nproc)
  #     - name: Pack
  #       run: |
  #         mkdir -p build/dist 
  #         cd build/bin/
  #         zip ../dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-intel-mkl-x64.zip ./*
  #     - name: Upload Artifacts
  #       uses: actions/upload-artifact@v4
  #       with:
  #         name: intel-mkl-avx2-linux
  #         path: build/dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-intel-mkl-x64.zip
  # build_sycl_avx2_linux:
  #   needs: get_short_commit_hash
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Install Deps
  #       run: |
  #         wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
  #         echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | sudo tee /etc/apt/sources.list.d/oneAPI.list
  #         sudo apt-get update
  #         sudo apt-get install ccache intel-basekit intel-oneapi-mkl intel-oneapi-mkl-devel ninja-build zip -y
  #     - name: Checkout
  #       uses: actions/checkout@v4
  #       with:
  #         repository: OpenBMB/llama.cpp
  #         submodules: recursive
  #         ref: prepare-PR-of-minicpm-v2.5
  #     - name: Build
  #       run: |
  #         mkdir build
  #         cd build
  #         source /opt/intel/oneapi/setvars.sh
  #         cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_SYCL=ON -DLLAMA_SYCL_F16=ON -DLLAMA_AVX2=ON
  #         cmake --build . --config Release -j $(nproc)
  #     - name: Pack
  #       run: |
  #         mkdir -p build/dist
  #         cd build/bin/
  #         mv ../*.so ./
  #         mv ../build/examples/**/*.so ./
  #         zip ../dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-sycl-x64.zip ./*
  #     - name: Upload Artifacts
  #       uses: actions/upload-artifact@v4
  #       with:
  #         name: sycl-avx2-linux
  #         path: build/dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-sycl-x64.zip
  build_cu121_avx2_linux:
    needs: get_short_commit_hash
    runs-on: ubuntu-latest
    steps:
      - name: Install CUDA 12.1
        id: install-cuda
        uses: Jimver/cuda-toolkit@v0.2.16
        with:
          cuda: "12.1.1"
          method: network
      - name: Install Deps
        run: sudo apt-get install ccache ninja-build zip -y
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: OpenBMB/llama.cpp
          submodules: recursive
          ref: prepare-PR-of-minicpm-v2.5
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON -DLLAMA_CUBLAS=ON -DLLAMA_CUDA_F16=true -DLLAMA_AVX2=ON
          cmake --build . --config Release -j $(nproc)
      - name: Pack
        run: |
          mkdir -p build/dist
          cd build/bin/
          mv ../*.so ./
          mv ../build/examples/**/*.so ./
          zip ../dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-cublas-cu121-x64.zip ./*
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cu121-avx2-linux
          path: build/dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-linux-avx2-cublas-cu121-x64.zip
  # build_cu121_avx512_win:
  #   needs: get_short_commit_hash
  #   runs-on: windows-latest
  #   steps:
  #     - name: Setup VS Dev Environment
  #       uses: seanmiddleditch/gha-setup-vsdevenv@v4
  #     - name: Install CUDA 12.1
  #       id: install-cuda
  #       uses: Jimver/cuda-toolkit@v0.2.16
  #       with:
  #         cuda: "12.1.1"
  #         method: network
  #         sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'
  #     - name: Setup ccache
  #       uses: Chocobo1/setup-ccache-action@v1
  #       with:
  #         windows_compile_environment: msvc
  #     - name: Install Ninja
  #       uses: urkle/action-get-ninja@v1
  #     - name: Checkout
  #       uses: actions/checkout@v4
  #       with:
  #         repository: OpenBMB/llama.cpp
  #         submodules: recursive
  #         ref: prepare-PR-of-minicpm-v2.5
  #     - name: Build
  #       run: |
  #         mkdir build
  #         cd build
  #         cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DLLAMA_CUDA=ON -DLLAMA_CUDA_F16=true -DLLAMA_AVX512=ON
  #         cmake --build . --config Release -j $env:NUMBER_OF_PROCESSORS
  #     - name: Pack
  #       run: |
  #         cd build\bin\
  #         7z a ..\dist\llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx512-cublas-cu121-x64.zip ./*
  #     - name: Upload Artifacts
  #       uses: actions/upload-artifact@v4
  #       with:
  #         name: cu121-avx512-win
  #         path: build/dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx512-cublas-cu121-x64.zip
  # build_cu113_avx2_win:
  #   needs: get_short_commit_hash
  #   runs-on: windows-2019
  #   steps:
  #     - name: Setup VS Dev Environment
  #       uses: seanmiddleditch/gha-setup-vsdevenv@v4
  #     - name: Install CUDA 11.3
  #       id: install-cuda
  #       uses: Jimver/cuda-toolkit@v0.2.16
  #       with:
  #         cuda: "11.3.1"
  #         method: network
  #         sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'
  #     - name: Setup ccache
  #       uses: Chocobo1/setup-ccache-action@v1
  #       with:
  #         windows_compile_environment: msvc
  #     - name: Install Ninja
  #       uses: urkle/action-get-ninja@v1
  #     - name: Checkout
  #       uses: actions/checkout@v4
  #       with:
  #         repository: OpenBMB/llama.cpp
  #         submodules: recursive
  #         ref: prepare-PR-of-minicpm-v2.5
  #     - name: Build
  #       run: |
  #         mkdir build
  #         cd build
  #         cmake .. -G "Ninja" -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DLLAMA_CUDA=ON -DLLAMA_CUDA_KQUANTS_ITER=1 -DCMAKE_CUDA_ARCHITECTURES="35" -DLLAMA_AVX2=ON
  #         cmake --build . --config Release -j $env:NUMBER_OF_PROCESSORS
  #     - name: Pack
  #       run: |
  #         cd build\bin\
  #         7z a ..\dist\llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx2-cublas-cu113-x64.zip ./*
  #     - name: Upload Artifacts
  #       uses: actions/upload-artifact@v4
  #       with:
  #         name: cu113-avx2-win
  #         path: build/dist/llama-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}-bin-win-avx2-cublas-cu113-x64.zip
  release:
    runs-on: ubuntu-latest
    # needs: [get_short_commit_hash, build_intel_mkl_avx2_linux, build_sycl_avx2_linux, build_cu121_avx2_linux, build_cu121_avx512_win, build_cu113_avx2_win]
    needs: [get_short_commit_hash, build_intel_mkl_avx2_linux, build_sycl_avx2_linux, build_cu121_avx2_linux]
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: |
            dist/
      - name: Prepare Release
        run: |
          cd dist
          mv -f intel-mkl-avx2-linux/* .
          mv -f sycl-avx2-linux/* .
          mv -f cu121-avx2-linux/* .
          mv -f cu121-avx512-win/* .
          mv -f cu113-avx2-win/* .
          rm -rf intel-mkl-avx2-linux sycl-avx2-linux cu121-avx2-linux
      - name: Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: llama_cpp-minicpm-v2.5-${{ needs.get_short_commit_hash.outputs.hash }}
          name: Binary build for llama.cpp (MiniCPM-V2.5-${{ needs.get_short_commit_hash.outputs.hash }})
          body: |
            This release contains the x86_64 binary (MiniCPM-V2.5 version) of [llama.cpp](https://github.com/OpenBMB/llama.cpp/tree/prepare-PR-of-minicpm-v2.5).
          files: dist/*
          fail_on_unmatched_files: true
